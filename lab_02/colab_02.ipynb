{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c677c949-8c05-4389-9265-4e03d09e41e0",
   "metadata": {},
   "source": [
    "<img align=\"right\" style=\"max-width: 200px; height: auto\" src=\"./sfao_logo.jpg\">\n",
    "\n",
    "##  Lab 02 - Foundation Model Audit Models\n",
    "\n",
    "Januar Schulung, Eidgenössische Finanzkontrolle (EFK), 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1b4e9c-f597-44bd-964f-81719ab8462e",
   "metadata": {},
   "source": [
    "Die Labs zur **Künstlichen Intelligenz** der Januar Schulung 2025 basieren auf Jupyter Notebook. Anhand solcher Notebooks ist es möglich eine Vielzahl von Datenanalysen und statistischen Validierungen durchzuführen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5aa882-8c1b-40b7-8e6f-fcec18d7150e",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 700px\" src=\"./lab_02_banner.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f18dcc-2518-43a5-858c-45d4caadc772",
   "metadata": {},
   "source": [
    "In diesem Lab werden wir untersuchen, wie Foundation Modelle in Prüfungsaufgaben integriert werden können, um komplexe Prozesse effizient zu automatisieren und zu analysieren. Ziel ist es, ein vortrainiertes Modell an spezifische Aufgaben anzupassen, wie z. B. die Analyse von Buchhaltungsdaten, die Anomalieerkennung und die Textauswertung von auditrelevanten Dokumenten. Dieser Ansatz baut auf dem Konzept des [**AI Co-Piloted Auditing**](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4444763), auf, bei dem Foundation Modelle vielseitige Aufgaben übernehmen und durch ihre Fähigkeit zur Sprachverarbeitung und Mustererkennung einen Mehrwert schaffen. Die Modelle agieren nicht isoliert, sondern als Werkzeuge, die den Prüfungsprozess unterstützen und die Arbeit menschlicher Prüfer ergänzen. Die folgende Abbildung veranschaulicht den Prüfungsprozess, den wir in diesem Lab mithilfe eines Foundation Modells umsetzen möchten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2e191d-4e03-450c-84fb-51630358afbd",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 1000px\" src=\"./audit_process.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f938e0d-c837-40e2-bf6e-844a99395621",
   "metadata": {},
   "source": [
    "Mit [**LangChain**](https://www.langchain.com/) können wir komplexe **Chain-of-Thought-Prozesse** modellieren, die es ermöglichen, Foundation Modelle gezielt für die Analyse von Buchhaltungsbuchungen, die Anomalieerkennung und die Dokumentenauswertung einzusetzen. Im Gegensatz zu CrewAI, das stärker auf agentenbasierte Architekturen ausgerichtet ist, bietet LangChain ein flexibles Werkzeugset, um Foundation Modelle auf spezifische Anforderungen im Auditbereich zu adaptieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a25966c-2659-4ed0-8f8b-0c22bfe11b8f",
   "metadata": {},
   "source": [
    "Mit dem `LangChain` Framework werden wir einen **Chain-of-Thought Prozess** (wie oben dargestellt) implementieren und bereitstellen, um Buchhaltungsbuchungen gemäss dem [International Standard on Auditing (ISA) 240](https://www.ifac.org/_flysystem/azure-private/publications/files/A012%202013%20IAASB%20Handbook%20ISA%20240.pdf) zu prüfen. Durch den Chain-of-Thought Prozess wird sichergestellt, dass die einzelnen Schritte des Prüfungsprozesses logisch aufeinander aufbauen, wodurch eine klare und nachvollziehbare Struktur für die Entscheidungsfindung entsteht."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ada38a-62ad-4800-b40d-265b70733f5c",
   "metadata": {},
   "source": [
    "Bei etwaigen Fragen wenden Sie sich, wie immer gerne an uns via **marco (dot) schreyer (at) efk (dot) admin (dot) ch**. Wir wünschen Ihnen Viel Freude mit unseren Notebooks und Ihren revisorischen Analysen!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3441c5-bc7f-4ca3-8288-52ed9005b529",
   "metadata": {},
   "source": [
    "## Lernziele des Labs:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b483d507-7bc7-4347-844a-f0d6dcd3a9c9",
   "metadata": {},
   "source": [
    "Nach dem heutigen Lab sollten Sie in der Lage sein:\n",
    "\n",
    "> 1. **Erstellung von Prompts:** Definition und Organisation von Task-, Action-, Input- und Output-Prompts. \n",
    "> 2. **Chain-of-Thought Prompting:** Nutzung von Chain-of-Thought zur Strukturierung von Prüfungsaufgaben.  \n",
    "> 3. **Einsatz von LangChain:** Anwendung des LangChain-Frameworks zur Verarbeitung von Prompts.  \n",
    "> 4. **Prüfungsworkflow:** Umsetzung eines Chain-of-Thought Prozesses für das Journal Entry Testing. \n",
    "\n",
    "Am Ende des Labs werden Sie verstehen, wie Sie mit Foundation Modellen und der `LangChain` Bibliothek Chain-of-Thought Prozesse einrichtet um Prüfprozesse zunehmend zu automatisieren. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf28db5-bdec-43eb-a14e-2c494c3fb29e",
   "metadata": {},
   "source": [
    "## 1. Einrichten der Analyseumgebung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136f50dc-03da-4c24-ad16-d135eeff2860",
   "metadata": {},
   "source": [
    "Ähnlich wie in den vorangegangenen Übungen werden wir zunächst eine Reihe von Python-Bibliotheken importieren, welche die Datenanalyse und -visualisierung ermöglichen. In dieser Übung werden wir die Bibliotheken `Langchain`, `Pandas`, `Numpy`, `Matplotlib` und `Seaborn` verwenden. Nachfolgend importieren wir die benötigten Bibliotheken durch die Ausführung der folgenden Anweisungen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01615f89-8d07-4005-830d-3efd2e7ef9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python data science and utility libraries\n",
    "import os, io, requests, warnings\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d81502f-96bf-482b-9112-b9b22156478a",
   "metadata": {},
   "source": [
    "Ausschalten möglicher Warnmeldungen z.B. aufgrund von zukünftigen Änderungen der Bibliotheken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ec07ce-9e8c-4dcc-8b64-511404ef019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the warning filter flag to ignore warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0a1db8-d6b5-4fe6-9409-447cdcc2b376",
   "metadata": {},
   "source": [
    "Installation der `PyPDF` und `LangChain` Bibliotheken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dd69e2-5149-41aa-bfdd-0cff6381528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip -qqq install PyPDF2\n",
    "! pip -qqq install langchain\n",
    "! pip -qqq install langchain-openai\n",
    "! pip -qqq install langchain-ollama\n",
    "! pip -qqq install langchain-huggingface\n",
    "! pip -qqq install langchain-community\n",
    "! pip -qqq install faiss-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329b365d-8b09-4223-9514-1698e65b3506",
   "metadata": {},
   "source": [
    "Import der `PyPDF` PDF Analyse Bibliotheken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f161637-06b4-4ced-8601-efa660894f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pypdf document reader libraries\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda496ce-7e88-4640-a0c4-3b754f4fdd13",
   "metadata": {},
   "source": [
    "Import der `Langchain` LLM Building Bibliotheken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eaa444-55c1-4e97-b36f-c1633da0cc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import langchain llm building libraries\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc75c21-962f-4a83-b744-5f1c8096f04c",
   "metadata": {},
   "source": [
    "Import der `Langchain`LLM Callback Bibliotheken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bcbd1d-be90-49f3-9560-773c3fe05b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import langchain llm callback libraries\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474007c2-5e04-40dc-b32f-b2f7fe2ae71f",
   "metadata": {},
   "source": [
    "Import der `Langchain`LLM Chain Bibliotheken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bc0769-1df6-4f5e-ae1b-c87e43ba4894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import langchain llm chain libraries\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929d6edf-d06f-4a05-a678-70818eaf5396",
   "metadata": {},
   "source": [
    "Import der `Langchain`LLM Vectorstore Bibliotheken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9796dddc-a390-4465-923e-54e9793eccca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import langchain llm vectorstore libraries\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa59bd9-714c-4785-99b3-4413a225c008",
   "metadata": {},
   "source": [
    "## 2. Einrichtung des Large Language Models (LLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8347cf-a495-4eb7-a903-503cd3bad096",
   "metadata": {},
   "source": [
    "In diesem Abschnitt richten wir lokal gehostete Large Language Models (LLMs) lokal ein, welche über die [**Ollama**](https://ollama.com/) Anwendung bereitgestellt werden. Hierduch ist es uns möglich verschiedene LLMs für Prüfungsaufgaben zu evaluieren. Darüber hinaus wir verwenden wir die **Langchain**-Schnittstelle `Ollama`, um auf unterschiedliche Foundation Modelle bzw. LLMs zuzugreifen. Die **Ollama**-Anwendung bietet die Möglichkeit, LLMs lokal auszuführen. Wir werden das **LLaMA3**-Modell von Meta.ai über **Ollama** einrichten und verwenden. Dies ermöglicht eine lokale Ausführung des Modells, reduziert die Latenz und verbessert den Datenschutz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5d9438-a747-46b3-beed-819456ff5b5e",
   "metadata": {},
   "source": [
    "Dazu installieren wir zunächst **Ollama** in der Google Colab-Umgebung. Zuerst müssen wir unser Colab-Notebook so einrichten, dass es Befehle über die Kommandozeile unterstützt. Der nachfolgende Code installiert entsprechend die `colab-xterm`-Bibliothek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151e3a08-f583-4bd0-ae9e-c53f1d82deaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip -qqq install colab-xterm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985c839b-11bf-4f70-bd07-d9f917f9fb81",
   "metadata": {},
   "source": [
    "Anschliessend aktivieren wir die `Colab XTerm`-Erweiterung, wodurch wir Shell-Befehle direkt im Notebook ausführen können:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed3123e-298c-4f75-ad1c-859960aafdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext colabxterm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ff1ed3-7775-48b0-bb49-1be51f0655bc",
   "metadata": {},
   "source": [
    "Nun starten wir das xterm-Terminal innerhalb unserer Colab-Zelle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0f4c8d-a032-4227-a18b-c5698ebfea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "%xterm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a74325-c934-4d5f-aa6a-70fd8e1f8869",
   "metadata": {},
   "source": [
    "Nachdem das xterm geöffnet ist, können wir mit der Installation von Ollama fortfahren. Führen Sie die folgenden Befehle im Terminal aus, um **Ollama** zu installieren:\n",
    "\n",
    "> curl https://ollama.ai/install.sh | sh\n",
    "\n",
    "Dieser Befehl lädt das Installationsskript von der Ollama-Website herunter und führt es aus. Das Skript übernimmt den Installationsprozess automatisch, einschließlich des Herunterladens und Installierens notwendiger Abhängigkeiten. Nach der Installation können wir den Server mit folgendem Befehl starten:\n",
    "\n",
    "> ollama serve &\n",
    "\n",
    "Das `&` am Ende lässt den Befehl im Hintergrund laufen, sodass das Terminal weiter genutzt werden kann."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c359126e-a1c7-4850-a95b-307569f3fd7f",
   "metadata": {},
   "source": [
    "Nach erfolgreicher Installation führen wir den Befehl `ollama list` aus, um die verfügbaren Modelle anzuzeigen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3bc401-1309-4fe7-86ff-ae6e5de6125b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b53df8-7dea-44ce-a270-e7a79afc347a",
   "metadata": {},
   "source": [
    "Unser Ziel ist es, das **LLaMA 3.2**-Modell zu verwenden, das instruktionstauglich ist und über **3 Milliarden Parameter** verfügt (Text-in/Text-out). Dieses Modell wurde für mehrsprachige Dialogfälle optimiert, einschließlich agentenbasierter Such- und Zusammenfassungsaufgaben. Wir laden das **LLaMA 3.2**-Modell in der `Google Colab`-Anwendung mit dem `ollama pull`-Befehl herunter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e2c09e-6001-454b-9cb9-76fde2085933",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ollama pull llama3.2:3b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3fcfbe-4def-46d5-b83a-cff7d41fe954",
   "metadata": {},
   "source": [
    "Anschließend überprüfen wir kurz, ob das Modell erfolgreich heruntergeladen wurde:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c8508e-584c-4ba4-8db1-02f821bec62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaf4fe4-de68-4637-938e-42e2dd6a22a8",
   "metadata": {},
   "source": [
    "Perfekt! Jetzt führen wir das Modell lokal auf Ihrem Gerät mit dem Befehl `ollama run` und dem entsprechenden Modellnamen aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9a9c62-5372-4514-8230-886ca106404b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ollama run llama3.2:3b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edebd013-5c1d-4188-93a4-1ef90e3a1fe1",
   "metadata": {},
   "source": [
    "Da das Modell jetzt läuft, verwenden wir die **Langchain**-Schnittstelle `ChatOpenAI`, um eine Verbindung zu dem lokal gehosteten **LLaMA 3.2**-Modell herzustellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d677c3b-18b2-407c-b02d-fee101ca8398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the local LLaMA 3.2 by meta.ai\n",
    "llm = OllamaLLM(model='llama3.2:3b', temperature=0.7, callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e675f5-5a80-4b99-be4a-10db699a298c",
   "metadata": {},
   "source": [
    "Als Nächstes testen wir das lokale **LLaMA3**-Modell. Wir simulieren ein Szenario, in dem die KI aufgefordert wird, einen Spesenbericht eines etwas eigenwilligen Mitarbeiters zu prüfen, der möglicherweise private Ausgaben einreichen möchte. Dieser Test stellt sicher, dass das Modell korrekt reagiert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19845ad1-6be1-49d1-991b-00e145558d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a fun audit prompt \n",
    "prompt = \"\"\"\n",
    "Imagine you're an auditor reviewing an expense report from an employee who claims all expenses are legitimate business costs. \n",
    "Here's a list of items they've submitted:\n",
    "\n",
    "1. Office supplies - $100\n",
    "2. Coffee machine - $250\n",
    "3. Unicorn onesie - $45\n",
    "4. 10-pound gummy bear - $30\n",
    "5. \"Business trip\" to Disneyland - $2000\n",
    "6. Sushi for team meeting - $150\n",
    "\n",
    "Please identify which items are legitimate business expenses and which might require further investigation, with a bit of humor in your response. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db6b4ff-52e7-4804-84b9-a9433c769302",
   "metadata": {},
   "source": [
    "Das lokale **LLaMA3**-Modell wird beauftragt, zu bestimmen, welche Posten im Spesenbericht legitim sind und welche einer genaueren Prüfung bedürfen. Das Modell sollte eine humorvolle, aber praktische Analyse der Ausgaben liefern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e618ba-a443-4a91-a069-5dbdd878d6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model with this fun audit prompt\n",
    "response = llm(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453f8ee0-5f07-4d1c-9bf8-6b06a34b33bc",
   "metadata": {},
   "source": [
    "In diesem Abschnitt haben wir eine Möglichkeiten erkundet, Sprachmodelle in unser Prüfungstoolkit zu integrieren: die Einrichtung eines lokalen Modells mit **Ollama** und das absetzen von einfachen Prompts. Diese Tools bieten Flexibilität bei der Durchführung Foundation Model basierter Prüfungsaufgaben. Mit dieser eingerichteten Umgebung sind wir bereit, uns praktischen Anwendungen zuzuwenden, bei denen diese Modelle als Co-Piloten in einem definierten Prüfungsprozess eingesetzt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e239d0-ec19-480b-9faf-cb6138a6d591",
   "metadata": {},
   "source": [
    "## 3. Foundation Modell basierte Prüfung von Journal Entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeb5d24-e6fb-4036-85bc-f1da857d2719",
   "metadata": {},
   "source": [
    "In diesem Abschnitt wird die praktische Anwendung des **Chain-of-Thought Promptings** im Prüfungsprozess am Beispiel des Journal Entry Testings demonstriert. Ziel ist es, die definierten Prompts zu nutzen, um strukturierte und nachvollziehbare Ergebnisse zu generieren. Der Fokus liegt auf der schrittweisen Verarbeitung der Prompts, von der Aufgabenbeschreibung bis zur Ergebnisgenerierung."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13389bea-b70c-41da-813d-8cf2e0ac8193",
   "metadata": {},
   "source": [
    "### 3.1 Definition der Chain-of-Thoughts Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d55395-0533-4b76-928c-4759c7ead5f7",
   "metadata": {},
   "source": [
    "Das **Chain-of-Thought Prompting** beschreibt eine Technik, die es ermöglicht, komplexe Aufgaben in logisch aufeinanderfolgende Schritte zu unterteilen. Durch diese Methode kann ein Modell nicht nur Ergebnisse generieren, sondern auch seinen Denkprozess und die angewendeten Schritte erklären. Dies ist besonders wertvoll im Audit-Kontext, wo Transparenz und Nachvollziehbarkeit essenziell sind.\n",
    "\n",
    "Im Folgenden verwenden wir das **Chain-of-Thought Prompting**, um die Fähigkeiten des Foundation Modells systematisch zu strukturieren. Hierbei werden verschiedene Arten von Prompts definiert, die jeweils unterschiedliche Aspekte der Prüfungsaufgabe ansprechen. Die folgende Untergliederung beschreibt, wie diese Prompts gestaltet werden können, um das Modell durch den **Journal Entry Testing Prüfprozess** zu führen – von der Kontextsetzung über die Datenverarbeitung bis hin zur Identifikation von Buchungsanomalien. Jedes Prompt hat eine spezifische Funktion und trägt dazu bei, den Prüfungsprozess transparent und effizient zu gestalten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc3c6b6-0123-42fd-a64d-48c23a13dc89",
   "metadata": {},
   "source": [
    "#### 3.1.1 Definition des Task-Explanation Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431137b4-15eb-46cd-b724-a065f74c7336",
   "metadata": {},
   "source": [
    "Das nachfolgende Prompt legt den Kontext für die durchzuführende Prüfungsaufgabe fest, z. B. die Prüfung von Buchhaltungsbuchungen, Audit-Sampling oder die Analyse von Anmerkungen. Darüber hinaus wird die Rolle erläutert, die das Foundation Modell übernehmen soll. Um mehrere Prompts zu ermöglichen, enthält es einen Satz, der das Modell anweist, auf das nächste Prompt zu warten und keine Aktionen eigenständig zu initiieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f130f86-dc57-4b8e-ac92-075da8bed27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_explanation_prompt():\n",
    "    return \"\"\"\n",
    "    You are an excellent auditor of tabular financial transactions\n",
    "    als referred to as \"journal entries\" . Given a tabular journal entry\n",
    "    dataset and information about how to audit the data, you break it\n",
    "    down into a sequence of audit actions. \n",
    "    \n",
    "    Please set this as Rule 1 and obey this rule for all following instructions \n",
    "    unless you are asked to ignore the rule.\n",
    "\n",
    "    Please do not start auditing until I say \"Audit\" and don't reply anything\n",
    "    else. Instead, just return the output message \"Processed - Waiting\n",
    "    for next input.\" and nothing else.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b400932f-6086-4701-9f26-2f7b9aa1b51c",
   "metadata": {},
   "source": [
    "#### 3.1.2 Definition des Action-Explanation Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344e3b22-ba81-4877-8491-1bc98e581053",
   "metadata": {},
   "source": [
    "Das nachfolgende Prompt definiert und beschreibt die erforderlichen Co-Pilot-Aktionen, die zur Lösung einer bestimmten Aufgabe notwendig sind, z. B. Attribut-Korrelationsanalyse, Textsentiment-Analyse oder Bildsegmentierung. Die Auswahl der geeigneten Co-Pilot-Aktionen hängt von der Prüfungsaufgabe ab und ermöglicht die Vorbereitung einer notwendigen und hinreichenden Menge von Prüferaktionen zur Manipulation von Auditdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b757a8-4eee-44db-8765-3210399432ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_explanation_prompt():\n",
    "    return \"\"\"\n",
    "    As an auditor, your task is to examine journal entry data\n",
    "    recorded in an audit client's Enterprise Resource Planning (ERP)\n",
    "    system. Specifically, you are responsible for detecting two types of\n",
    "    anomalous journal entries within large datasets:\n",
    "\n",
    "    Anomalies are classified into two categories:\n",
    "\n",
    "    1. Global Anomalies: These anomalies involve an unusual combination of\n",
    "    values for any single attribute when compared to the entire dataset.\n",
    "    For example, a significantly higher or lower transaction amount for a\n",
    "    specific department title or a rare department title in the dataset\n",
    "    would be considered a global anomaly. Global anomalies are often\n",
    "    related to an attribute that stands out across the entire dataset,\n",
    "    regardless of other attributes' values.\n",
    "\n",
    "    2. Local Anomalies: These anomalies involve an unusual combination of\n",
    "    values for a specific vendor and document type when compared to other\n",
    "    entries with the same vendor and document type. For example, a rare\n",
    "    department title found only within a specific vendor and document\n",
    "    type combination, or an unusual transaction amount for that\n",
    "    combination would be considered a local anomaly. Local anomalies are\n",
    "    detected within a subset of the dataset, considering the context of\n",
    "    similar entries.\n",
    "\n",
    "    Keep these definitions in mind when analyzing datasets for anomalies. As\n",
    "    an auditor, your goal is to detect both types of journal entry\n",
    "    anomalies.\n",
    "\n",
    "    Please set this as Rule 2 and obey this rule for all following instructions \n",
    "    unless you are asked to ignore the rule.\n",
    "\n",
    "    Please do not start auditing until I say \"Audit\" and don't reply anything\n",
    "    else. Instead, just return the output message \"Processed - Waiting\n",
    "    for next input.\" and nothing else.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ae0ba7-38d5-4496-b150-c51adb13ed6d",
   "metadata": {},
   "source": [
    "#### 3.1.3 Definition des Input-Data-Explanation Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f80830-6b61-4058-88f3-9ce65d2cc3f4",
   "metadata": {},
   "source": [
    "Das nachfolgende Prompt definiert die Eingabedaten, die für die Prüfungsaufgabe verarbeitet werden sollen. Es enthält eine allgemeine Beschreibung der zu verarbeitenden Datenmodalität, z. B. tabellarische, textuelle oder Bilddaten. Darüber hinaus umfasst es eine detaillierte Beschreibung der Eingabedaten, z. B. Attributnamen, Attributsemantik, Inhaltsverzeichnis und lexikalische Informationen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85652f1f-1c54-4b83-9dd5-40ed0aaa421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_data_explanation_prompt():\n",
    "    return \"\"\"\n",
    "    Enterprise Resource Planning (ERP) systems generally store\n",
    "    journal entries in a tabular database table format. This table will\n",
    "    serve as the input data for analyzing global and local anomalies.\n",
    "    Each row in the table represents a single journal entry, while each\n",
    "    of the six columns represents a distinct journal entry attribute. The\n",
    "    six columns are defined as follows:\n",
    "    \n",
    "    1. Fiscal_Year: The fiscal year during which the journal entry was\n",
    "    posted, running from July 1st to June 30th.\n",
    "    2. Document_Number: A unique identifier for each journal entry, used for\n",
    "    tracking individual entries within the organization's ERP system.\n",
    "    3. Department_Title: The department title used in the organization's\n",
    "    finance and accounting system that denotes the department that posted\n",
    "    the journal entry.\n",
    "    4. Vendor_Name: The name of the individual, vendor, or entity associated\n",
    "    with the journal entry.\n",
    "    5. Document_Type: The classification of the journal entry within the\n",
    "    organization's ERP system.\n",
    "    6. Transaction_Amount: The transaction or payment amount in USD.\n",
    "\n",
    "    The table may contain a large, yet arbitrary, number of journal entries,\n",
    "    depending on the audit client's business activity. The table data is\n",
    "    formatted as a comma-separated values (CSV) file. Below is a sample\n",
    "    of a database table in CSV format, with the first row representing\n",
    "    attribute names and the following rows representing individual\n",
    "    journal entries.\n",
    "    \n",
    "    Input Data:\n",
    "    \n",
    "    Fiscal_Year, Document_Number, Department_Title, Vendor_Name, Document_Type, Transaction_Amount\n",
    "    2017, CHEK17119771, 42 COMMERCE, EAT AT JOE'S, petty cash, 50.00\n",
    "    2017, CHEK17119723, 26 LICENSES & INSPECTIONS, MARLENE BELL REPORTING INC., procurement, 454.20\n",
    "    2017, CHEK17119939, 44 LAW, RICOH AMERICAS CORPORATION, payment voucher, 127.33\n",
    "    ... \n",
    "\n",
    "    Please set the description above as Rule 3 and obey this rule for all\n",
    "    following instructions unless you are asked to ignore the rule.\n",
    "    \n",
    "    Please do not start auditing until I say \"Audit\" and don't reply anything\n",
    "    else. Instead, just return the output message \"Processed - Waiting\n",
    "    for next input.\" and nothing else.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209ec854-030e-4fb2-8814-c9df19175a6c",
   "metadata": {},
   "source": [
    "#### 3.1.4 Definition des Output-Data-Explanation Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14130397-5807-4532-8cb6-222dc276a850",
   "metadata": {},
   "source": [
    "Das nachfolgende Prompt definiert die Ausgabedaten, die zur Lösung der Prüfungsaufgabe generiert werden. Es enthält eine allgemeine Beschreibung der zu generierenden Datenmodalität, z. B. tabellarische, textuelle oder Bilddaten. Darüber hinaus umfasst es eine detaillierte Beschreibung der Ausgabedaten, z. B. Warnmeldungen, Dateiformate der Ausgabe und Visualisierungen der Ergebnisse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7d2922-daaa-4722-8350-4c8162e66e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_data_explanation_prompt():\n",
    "    return \"\"\"\n",
    "    In general, anomaly detection systems provide two types of\n",
    "    information after successfully analyzing journal entries within a\n",
    "    tabular database table. The first type of information is the detected\n",
    "    anomalous journal entries themselves, and the second type is an\n",
    "    explanation of why each entry was identified as an anomaly. Below is\n",
    "    an example of an alert message format that should be generated when a\n",
    "    local or global anomaly is detected:\n",
    "\n",
    "    Alert Message!\n",
    "    Detected Global Anomaly:\n",
    "    [Placeholder for the detected anomalous journal entries.]\n",
    "\n",
    "    Dectection Rationale:\n",
    "    [Placeholder for the rationale of why the journal entry was detected as an anomaly.]\n",
    "\n",
    "    If multiple global anomalies are detected, separate alert messages should\n",
    "    be generated for each anomaly. If you find you cannot find any\n",
    "    anomalies, please explain the reason. \n",
    "    \n",
    "    Please set the description above as Rule 4 and obey this rule for all \n",
    "    following instructions unless you are asked to ignore the rule. \n",
    "    \n",
    "    Please do not start auditing until I say \"Audit\" and don't reply anything\n",
    "    else. Instead, just return the output message \"Processed - Waiting\n",
    "    for next input.\" and nothing else.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a1061c-79ce-4cfd-869a-5e45c3fa022f",
   "metadata": {},
   "source": [
    "#### 3.1.5 Definition des Global-Anomaly-Example Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad63c89a-c1c8-48af-a0dc-3a255cf811f6",
   "metadata": {},
   "source": [
    "Das nachfolgende Prompt dient der Definition von globalen Anomaliebeispielen, die für die Lösung der Prüfungsaufgabe generiert werden. Es enthält eine allgemeine Beschreibung der Datenmodalität, z. B. tabellarische, textuelle oder Bilddaten, sowie eine detaillierte Beschreibung der Beispiele, z. B. Muster, die globale Auffälligkeiten in den Daten widerspiegeln, und entsprechende Darstellungen der Ergebnisse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60190fb9-6963-48a0-9fc8-63c449431993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_anomaly_example_prompt():\n",
    "    return \"\"\"\n",
    "    In the following I'm providing you a global anomaly detection example.\n",
    "    \n",
    "    Input data:\n",
    "    \n",
    "    Fiscal_Year, Document_Number, Department_Title, Vendor_Name, Document_Type, Transaction_Amount\n",
    "    2017, CHEK17119393, 42 COMMERCE, EAT AT JOE'S, petty cash, 68.17\n",
    "    2017, CHEK17119771, 42 COMMERCE, EAT AT JOE'S, petty cash, 23.85\n",
    "    2017, CHEK17118549, 42 COMMERCE, EAT AT JOE'S, petty cash, 50.00\n",
    "    2017, CHEK17113292, 42 COMMERCE, EAT AT JOE'S, petty cash, 84.23\n",
    "    2017, CHEK17119504, 26 LICENSES & INSPECTIONS, EAT AT JOE'S, petty cash, 72.23\n",
    "    2017, CHEK17137637, 42 COMMERCE, EAT AT JOE'S, petty cash, 23.03\n",
    "    2017, CHEK17119893, 42 COMMERCE, EAT AT JOE'S, petty cash, 58.17\n",
    "    2017, CHEK13829239, 42 COMMERCE, EAT AT JOE'S, petty cash, 67.85\n",
    "    2017, CHEK17183932, 42 COMMERCE, EAT AT JOE'S, petty cash, 72.00\n",
    "    2017, CHEK17323423, 42 COMMERCE, EAT AT JOE'S, petty cash, 39.28\n",
    "    2017, CHEK17139393, 42 COMMERCE, EAT AT JOE'S, petty cash, 939.22\n",
    "    2017, CHEK17383833, 42 COMMERCE, EAT AT JOE'S, petty cash, 94.22\n",
    "    2017, CHEK17149494, 42 COMMERCE, EAT AT JOE'S, petty cash, 34.93\n",
    "    2017, CHEK17393939, 42 COMMERCE, EAT AT JOE'S, petty cash, 32.82\n",
    "    2017, CHEK17339594, 42 COMMERCE, EAT AT JOE'S, petty cash, 39.28\n",
    "    2017, CHEK17169595, 42 COMMERCE, EAT AT JOE'S, petty cash, 939.22\n",
    "    2017, CHEK17339203, 42 COMMERCE, EAT AT JOE'S, petty cash, 94.22\n",
    "    2017, CHEK17139203, 42 COMMERCE, EAT AT JOE'S, petty cash, 293.93\n",
    "    2017, CHEK17393829, 42 COMMERCE, EAT AT JOE'S, petty cash, 493.22\n",
    "    2017, CHEK17340309, 42 COMMERCE, EAT AT JOE'S, petty cash, 64.83\n",
    "    \n",
    "    And here is the expected output:\n",
    "    \n",
    "    Alert Message!\n",
    "    \n",
    "    Detected Global Anomaly:\n",
    "    \n",
    "    2017, CHEK17119504, 26 LICENSES & INSPECTIONS, EAT AT JOE'S, petty cash, 72.23\n",
    "    \n",
    "    Dectection Rationale:\n",
    "    The journal entry exhibting document number CHEK17119504 corresponds to a\n",
    "    global anomaly. The journal entry contains the department title \"26\n",
    "    LICENSES & INSPECTIONS\" which in unusual when compared to all other\n",
    "    journal entries in the dataset.\n",
    "\n",
    "    Please set the description above as Rule 5 and obey this rule for all \n",
    "    following instructions unless you are asked to ignore the rule. \n",
    "    \n",
    "    Please do not start auditing until I say \"Audit!\". Instead, just return\n",
    "    the output message \"Processed - Waiting for next input.\"\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78dcd18-2adf-48ac-9d31-c81f8eb0c7ce",
   "metadata": {},
   "source": [
    "#### 3.1.6 Definition des Local-Anomaly-Example Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fabc34-6755-4abd-a935-0383c3679e4e",
   "metadata": {},
   "source": [
    "Das nachfolgende Prompt definiert lokale Anomaliebeispiele, die spezifische Auffälligkeiten in den Daten identifizieren und darstellen. Es enthält eine allgemeine Beschreibung der Datenmodalität, z. B. tabellarische, textuelle oder Bilddaten, und liefert detaillierte Informationen zu den Beispielen, z. B. Auffälligkeiten auf Attributebene, Ausgabeformate und visuelle Darstellungen der identifizierten lokalen Anomalien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c07c905-d035-46b5-aa39-187b9f4edd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_anomaly_example_prompt():\n",
    "    return \"\"\"\n",
    "    In the following I'm providing you a local anomaly detection example.\n",
    "    \n",
    "    Input data:\n",
    "    \n",
    "    Fiscal_Year, Document_Number, Department_Title, Vendor_Name, Document_Type, Transaction_Amount\n",
    "    2017, CHEK17119393, 42 COMMERCE, EAT AT JOE'S, petty cash, 68.17\n",
    "    2017, ACHD17994939, 28 WATER, XEROX CORPORATION, procurement, 932.32\n",
    "    2017, CHEK17119771, 42 COMMERCE, EAT AT JOE'S, petty cash, 23.85\n",
    "    2017, CHEK17118549, 42 COMMERCE, EAT AT JOE'S, petty cash, 50.00\n",
    "    2017, CHEK17113292, 42 COMMERCE, EAT AT JOE'S, petty cash, 84.23\n",
    "    2017, ACHD17939323, 28 WATER, XEROX CORPORATION, procurement, 352.29\n",
    "    2017, CHEK17119504, 42 COMMERCE, EAT AT JOE'S, petty cash, 72.23\n",
    "    2017, CHEK17137637, 42 COMMERCE, EAT AT JOE'S, petty cash, 23.03\n",
    "    2017, ACHD17950344, 28 WATER, EAT AT JOE'S, petty cash, 292.22\n",
    "    2017, CHEK17119893, 42 COMMERCE, EAT AT JOE'S, petty cash, 58.17\n",
    "    2017, ACHD17830920, 28 WATER, XEROX CORPORATION, procurement, 93.92\n",
    "    2017, ACHD17994949, 28 WATER, XEROX CORPORATION, procurement, 833.59\n",
    "    2017, ACHD17390203, 28 WATER, XEROX CORPORATION, procurement, 26.59\n",
    "    2017, CHEK13829239, 42 COMMERCE, EAT AT JOE'S, petty cash, 67.85\n",
    "    2017, CHEK17183932, 42 COMMERCE, EAT AT JOE'S, petty cash, 72.00\n",
    "    2017, ACHD17300393, 28 WATER, XEROX CORPORATION, procurement, 252.19\n",
    "    2017, ACHD17193303, 28 WATER, XEROX CORPORATION, procurement, 68.38\n",
    "    2017, CHEK17323423, 42 COMMERCE, EAT AT JOE'S, petty cash, 39.28\n",
    "    2017, ACHD17930233, 28 WATER, XEROX CORPORATION, procurement, 103.39\n",
    "    2017, ACHD17392022, 28 WATER, XEROX CORPORATION, procurement, 632.84\n",
    "    \n",
    "    And here is the expected output:\n",
    "\n",
    "    Alert Message!\n",
    "    \n",
    "    Detected Local Anomaly:\n",
    "    \n",
    "    2017, ACHD17950344, 28 WATER, EAT AT JOE'S, petty cash, 292.22\n",
    "    \n",
    "    Dectection Rationale:\n",
    "    The journal entry exhibting document number ACHD17950344 corresponds to a\n",
    "    local anomaly. The journal entry contains the department title \"28\n",
    "    Water\" which is usually not observed with the vendor name \"EAT AT\n",
    "    JOE'S\" and document type \"petty cash\".\n",
    "\n",
    "    Please set the description above as Rule 6 and obey this rule for all \n",
    "    following instructions unless you are asked to ignore the rule.\n",
    "    \n",
    "    Please do not start auditing until I say \"Audit\" and don't reply anything\n",
    "    else. Instead, just return the output message \"Processed - Waiting\n",
    "    for next input.\" and nothing else.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83ae204-a04c-454a-a3db-87de84543c5c",
   "metadata": {},
   "source": [
    "### 3.2 Definition des Task Execution Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868a7207-2b64-410d-912f-a94cc6f02796",
   "metadata": {},
   "source": [
    "Die bisherigen Prompts haben das Modell Schritt für Schritt auf die eigentliche Prüfungsaufgabe vorbereitet. Der nachfolgende Prompt enthält die spezifischen Eingabedaten für die tatsächlich auszuführende Prüfungsaufgabe. Als finales Prompt weist es das Modell an, die Rolle eines Prüfers zu übernehmen und das zuvor durch die Chain-of-Thought-Prompts erlernte Wissen sowie die definierten Aktionen anzuwenden. Das Modell agiert nun als Co-Pilot, indem es die erlernte Aufgabe basierend auf den bereitgestellten Daten ausführt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6261d979-a7b8-405b-87e8-17081713e7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_execution_prompt():\n",
    "    return \"\"\"\n",
    "    Please analyze the following journal entries for both global\n",
    "    and local anomalies. For each anomaly detected, please generate an\n",
    "    alert message that includes the anomaly and an explanation for why it\n",
    "    was flagged. It is crucial to detect all anomalies present in the\n",
    "    dataset. Return all anomalies detected in a single response.\n",
    "\n",
    "    Fiscal_Year, Document_Number, Department_Title, Vendor_Name, Document_Type, Transaction_Amount\n",
    "    2017, CHEK17323423, 42 COMMERCE, EAT AT JOE'S, petty cash, 39.28\n",
    "    2017, CHEK19393092, 42 COMMERCE, EAT AT JOE'S, petty cash, 93.23\n",
    "    2017, CHEK17115653, 26 LICENCES, THE MCS GROUP, INC., petty cash, 68.17\n",
    "    2017, CHEK17119393, 42 COMMERCE, EAT AT JOE'S, petty cash, 68.17\n",
    "    2017, ACHD17144101, 25 FLEET MANAGEMENT, CHAPMAN FORD SALES, procurement, 932.32\n",
    "    2017, CHEK17119771, 26 LICENCES, THE MCS GROUP, INC., petty cash, 23.85\n",
    "    2017, CHEK17118549, 26 LICENCES, THE MCS GROUP, INC., petty cash, 50.00\n",
    "    2017, CHEK19959593, 26 LICENCES, THE MCS GROUP, INC., petty cash, 23.83\n",
    "    2017, CHEK17183822, 26 LICENCES, THE MCS GROUP, INC., petty cash, 42.00\n",
    "    2017, CHEK17137637, 42 COMMERCE, EAT AT JOE'S, petty cash, 23.03\n",
    "    2017, CHEK17139393, 42 COMMERCE, EAT AT JOE'S, petty cash, 293.23\n",
    "    2017, CHEK17113292, 26 LICENCES, THE MCS GROUP, INC., petty cash, 84.23\n",
    "    2017, ACHD17144101, 25 FLEET MANAGEMENT, CHAPMAN FORD SALES, procurement, 352.29\n",
    "    2017, CHEK17119504, 26 LICENCES, THE MCS GROUP, INC., petty cash, 72.23\n",
    "    2017, CHEK17119932, 42 COMMERCE, EAT AT JOE'S, petty cash, 19.75\n",
    "    2017, CHEK13829292, 42 COMMERCE, EAT AT JOE'S, petty cash, 28.79\n",
    "    2017, ACHD17234101, 25 FLEET MANAGEMENT, CHAPMAN FORD SALES, procurement, 222.20\n",
    "    2017, CHEK17132932, 42 COMMERCE, EAT AT JOE'S, petty cash, 20.72\n",
    "    2017, CHEK13842292, 42 COMMERCE, EAT AT JOE'S, petty cash, 22.34\n",
    "    2017, ACHD24454101, 25 FLEET MANAGEMENT, CHAPMAN FORD SALES, procurement, 493.19\n",
    "    2017, CHEK17137637, 26 LICENCES, THE MCS GROUP, INC., petty cash, 23.03\n",
    "    2017, ACHD17144101, 25 FLEET MANAGEMENT, EAT AT JOE'S, petty cash, 292.22\n",
    "    2017, CHEK17119771, 26 LICENCES, THE MCS GROUP, INC., petty cash, 23.85\n",
    "    2017, CHEK17113243, 26 LICENCES, THE MCS GROUP, INC., petty cash, 52.00\n",
    "    2017, CHEK17324423, 26 LICENCES, THE MCS GROUP, INC., petty cash, 93.17\n",
    "    2017, CHEK17119771, 42 COMMERCE, EAT AT JOE'S, petty cash, 1,000,000.00\n",
    "    2017, ACHD17145461, 25 FLEET MANAGEMENT, CHAPMAN FORD SALES, procurement, 93.92\n",
    "    2017, ACHD17144101, 25 FLEET MANAGEMENT, CHAPMAN FORD SALES, procurement, 833.59\n",
    "    2017, CHEK17118549, 42 COMMERCE, EAT AT JOE'S, petty cash, 50.00\n",
    "    2017, ACHD17143451, 25 FLEET MANAGEMENT, CHAPMAN FORD SALES, procurement, 26.59\n",
    "    2017, ACHD17144102, 25 FLEET MANAGEMENT, THE MCS GROUP, INC., petty cash, 229.59\n",
    "    2017, CHEK13829239, 26 LICENCES, THE MCS GROUP, INC., petty cash, 67.85\n",
    "    2017, CHEK17183932, 26 LICENCES, THE MCS GROUP, INC., petty cash, 72.00\n",
    "    2017, CHEK17394394, 26 FIREDEPARTMENT, EAT Even More AT JOESES's, petty cash, 72.23\n",
    "    2017, ACHD17434541, 25 FLEET MANAGEMENT, CHAPMAN FORD SALES, procurement, 252.19\n",
    "    2017, ACHD17144101, 25 FLEET MANAGEMENT, CHAPMAN FORD SALES, procurement, 68.38\n",
    "    2017, CHEK12022932, 42 COMMERCE, EAT AT JOE'S, petty cash, 66.72\n",
    "    2017, CHEK13322292, 42 COMMERCE, EAT AT JOE'S, petty cash, 93.33\n",
    "    2017, CHEK12023332, 42 COMMERCE, EAT AT JOE'S, petty cash, 83.77\n",
    "    2017, CHEK13343492, 42 COMMERCE, EAT AT JOE'S, petty cash, 192.33\n",
    "    2017, ACHD17545542, 25 FLEET MANAGEMENT, CHAPMAN FORD SALES, procurement, 34.64\n",
    "    2017, ACHD17132343, 25 FLEET MANAGEMENT, CHAPMAN FORD SALES, procurement, 29.32\n",
    "    2017, CHEK17323423, 26 LICENCES, THE MCS GROUP, INC., petty cash, 39.28\n",
    "    2017, CHEK17119893, 42 COMMERCE, EAT AT JOE'S, petty cash, 58.17\n",
    "    2017, CHEK13829239, 42 COMMERCE, EAT AT JOE'S, petty cash, 67.85\n",
    "    2017, CHEK29292932, 18 POLICE, DUNKIN DONUTS, petty cash, 72.00\n",
    "    2017, CHEK17113292, 42 COMMERCE, EAT AT JOE'S, petty cash, 84.23\n",
    "    2017, ACHD17144121, 25 FLEET MANAGEMENT, CHAPMAN FORD SALES, procurement, 103.39\n",
    "    2017, ACHD17144122, 25 FLEET MANAGEMENT, CHAPMAN FORD SALES, procurement, 632.84\n",
    "    2017, CHEK17113292, 42 COMMERCE, EAT AT JOE'S, petty cash, 34.73\n",
    "    2017, ACHD17144142, 25 FLEET MANAGEMENT, CHAPMAN FORD SALES, procurement, 342.50\n",
    "    2017, ACHD17144143, 25 FLEET MANAGEMENT, CHAPMAN FORD SALES, procurement, 294.23\n",
    "    2017, ACHD17144432, 25 FLEET MANAGEMENT, CHAPMAN FORD SALES, procurement, 43.53\n",
    "    2017, ACHD17324233, 25 FLEET MANAGEMENT, CHAPMAN FORD SALES, procurement, 4.23\n",
    "    \n",
    "    Audit!\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be48890a-7f1a-4454-9114-83b5f4af454f",
   "metadata": {},
   "source": [
    "### 3.3 Ausführen der Chain-of-Thought "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb601f3-4fef-4902-8355-96766343390b",
   "metadata": {},
   "source": [
    "Die Chain-of-Thought Methode ermöglicht es, komplexe Prüfungsaufgaben automatisiert in `LangChain` durch die schrittweise Anwendung logischer Schritte zu lösen. Mit LangChain und einem leistungsfähigen Language Model wie LLaMA können wir diese Schritte programmatisch umsetzen. Zunächst definieren wir hierzu die Chain-of-Thought Prompts, die den Prüfungsprozess strukturieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f50b121-f11f-4e18-80c9-1f8ce082c59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain-of-thought prompting creation\n",
    "chain_of_thought_prompts = {\n",
    "    \"task_explanation\": get_task_explanation_prompt(),\n",
    "    \"action_explanation\": get_action_explanation_prompt(),\n",
    "    \"input_data_explanation\": get_input_data_explanation_prompt(),\n",
    "    \"output_data_explanation\": get_output_data_explanation_prompt(),\n",
    "    \"global_example\": get_global_anomaly_example_prompt(),\n",
    "    \"local_example\": get_local_anomaly_example_prompt(),\n",
    "    \"task_execution\": get_task_execution_prompt()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b58961-8a2b-40a5-b7e9-1a428760dcd1",
   "metadata": {},
   "source": [
    "Weiter installieren wir ein noch leistungsstärkeres LLM, in diesem Fall das `LLaMA 3` von Meta AI mit insgesamt **8 Milliarden** Parametern. Dieses Modell ist entscheidend, da es die Verarbeitung und Ausführung der Prompts übernimmt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dc5908-1979-4c5e-9939-adeeee9c3530",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ollama pull llama3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991e63f8-dcb5-4491-9172-93d0acc4f5a0",
   "metadata": {},
   "source": [
    "Nach erfolgreicher Installation führen wir wieder den Befehl `ollama list` aus, um die verfügbaren Modelle anzuzeigen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7cb4ad-cda7-42d2-8014-f69b11b55016",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ca6293-56e9-4eae-a195-49d02e15d1c8",
   "metadata": {},
   "source": [
    "Wir konfigurieren das Modell mit spezifischen Parametern wie der Temperatur, die die Kreativität der Antworten steuert, und binden es an einen Callback Manager, um in Echtzeit Rückmeldungen zu erhalten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1163cfc-d539-44b1-a8e2-87c290eec515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the local LLaMA 3.2 by meta.ai\n",
    "llm = OllamaLLM(model='llama3:latest', temperature=0.9, callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a573dae-d655-4e9f-9692-30fd451a4555",
   "metadata": {},
   "source": [
    "Nachdem das Language Model initialisiert ist, werden die Prompts verarbeitet. Wir verwenden eine Schleife, um jedes Prompt aus dem `chain-of-thought-prompts` Dictionary zu laden und mithilfe der `LangChain` Bibliothek auszuführen.\n",
    "\n",
    "- **PromptTemplate:** Jedes Prompt wird als Vorlage geladen.\n",
    "- **LLMChain:** Eine Kette wird erstellt, die das Modell mit dem Prompt verbindet.\n",
    "- **Ausführung:** Das Modell verarbeitet das Prompt und generiert eine Antwort.\n",
    "- **Speicherung:** Die Antwort wird im `responses` Dictionary gespeichert.\n",
    "\n",
    "Dieser iterative Prozess ermöglicht es, die Ergebnisse jedes Schrittes festzuhalten und für die nächste Phase der Chain-of-Thought zu verwenden. Am Ende verfügen wir über eine Sammlung strukturierter Ergebnisse, die den gesamten Prüfungsprozess abbilden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cd1936-5f42-465f-b189-1a55d1c4e79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to store responses\n",
    "responses = {}\n",
    "\n",
    "# loop through each name and prompt in the chain-of-thoughts\n",
    "for name, prompt in chain_of_thought_prompts.items():\n",
    "    \n",
    "    # create a PromptTemplate object from the template string\n",
    "    prompt = PromptTemplate.from_template(prompt)\n",
    "    \n",
    "    # create an LLMChain using the Ollama LLM and the prompt\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    \n",
    "    # run the chain to get the response\n",
    "    response = chain.run({})\n",
    "    \n",
    "    # store the response in the dictionary with the corresponding name\n",
    "    responses[name] = response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723c9d0e-70da-44fe-8a8d-5ada8b9cb44b",
   "metadata": {},
   "source": [
    "Die zuvor ausgeführten Prompts haben Antworten und Erkenntnisse geliefert, die nun zu einem vollständigen Prüfungsbericht zusammengeführt werden können. Diese Ergebnisse repräsentieren eine strukturierte Analyse, die sowohl globale als auch lokale Anomalien berücksichtigt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4c4613-b82d-4d85-8e1d-0403048a752d",
   "metadata": {},
   "source": [
    "## 4. Foundation Modell basierte Prüfung von Berichten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6522bdf0-0a48-43c8-a06a-529d2b12747b",
   "metadata": {},
   "source": [
    "Dieses Kapitel beschreibt, wie ein Foundation-Modell verwendet werden kann, um Berichte effizient zu analysieren. Ziel ist es, große Textmengen zu strukturieren, zu verarbeiten und gezielt Fragen zum Inhalt mithilfe eines Modells beantworten zu können. Der Prozess gliedert sich in drei Hauptschritte: die Vorbereitung der Daten, die Initialisierung des Modells mit der Erstellung von Embeddings sowie die Durchführung einer Frage-Antwort-Analyse. \n",
    "\n",
    "Dabei greifen sogenannte **Retrieval-Augmented-Generation (RAG)** Systeme auf eine Kombination aus Wissensabruf und Textgenerierung zurück: Relevante Informationen werden aus einer Wissensbasis abgerufen und durch das Modell verarbeitet, um präzise und kontextbezogene Antworten zu liefern. Dieses Verfahren erlaubt es, auch bei umfangreichen und komplexen Datenquellen effizient und zielgerichtet Erkenntnisse zu gewinnen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4372dba1-4d90-425c-bd3a-acbdf9aafe2c",
   "metadata": {},
   "source": [
    "Die nachfolgende Abbildung zeigt den Ablauf eines **Retrieval-Augmented-Generation (RAG)** Systems. Eine Frage (Query) wird von einem Benutzer eingegeben, die relevante Informationen aus einem PDF-Dokument abruft. Diese Informationen werden in ein Foundation-AI-Modell eingespeist, das auf Basis des Kontextes eine präzise Antwort (Response) generiert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a578732-9ae0-4950-902c-865f3c90d256",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 800px\" src=\"./rag_process.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9153b1-43e8-4cc3-bf29-f0a14a1f8ea9",
   "metadata": {},
   "source": [
    "Nachfolgend möchten wir einen solches System beispielhaft implementieren. Zunächst ist es hierzu notwendig eine Vector-Datenbank der zu analysierenden Dokumente zu erstellen. Die Erstellung der Datenbank beginnt mit der Extraktion der Inhalte aus einem PDF-Dokument, wobei der Text in eine lesbare und verarbeitbare Form überführt wird. \n",
    "\n",
    "Anschliessend wird der Text in kleinere, überlappende Abschnitte, sogenannte Content Chunks, unterteilt, um den Kontext zwischen den Segmenten zu bewahren. Diese Textabschnitte werden mithilfe eines Embedding-Modells, in numerische Vektoren umgewandelt, die die semantische Bedeutung des Inhalts kodieren. Die resultierenden Embeddings, welche aus hochdimensionalen Zahlenwerten bestehen, werden in einer Vektor-Datenbank gespeichert. Diese Datenbank ermöglicht eine effiziente Ähnlichkeitssuche, wodurch relevante Informationen schnell und präzise abgerufen werden können. \n",
    "\n",
    "Das nachfolgende Schaubild illustriert diesen Prozess und zeigt die Transformation vom ursprünglichen Dokument über die semantischen Repräsentationen bis hin zur Speicherung in der Vektordatenbank."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7d7a73-62b2-4f94-97eb-10d14a2bac43",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 1000px\" src=\"./embedding_process.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1604336-d5a5-4c45-96b7-67595380e058",
   "metadata": {},
   "source": [
    "### 4.1 Extraktion der Berichtsinhalte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce269f6-8f38-4ddb-80e7-8c67d09810d0",
   "metadata": {},
   "source": [
    "Nun lernen wir, wie Inhalte aus Dokumenten ausgelesen werden können um sie anschliessend mit einem Foundation-Modell zu analysieren. Wir definieren den Pfad zum Dokument, welches auf GitHub im PDF-Format vorliegt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3098ed88-da26-490c-9fba-7dda7288d152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path to the PDF file\n",
    "pdf_path = 'https://raw.githubusercontent.com/GitiHubi/SFAO/master/lab_02/efk-jb-2023_extract.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d733476-6fa5-454c-a82d-ed3ef060c485",
   "metadata": {},
   "source": [
    "Im nächsten Schritt beziehen bauen wir eine Verbindung zu GitHub auf und beziehen das Dokument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417d8e4c-a3dd-4c00-95f3-ba634ac2d116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the PDF file from GitHub\n",
    "pdf_file = requests.get(pdf_path)\n",
    "\n",
    "# convert PDF file into a byte stream\n",
    "pdf_bytes = io.BytesIO(pdf_file.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c65319-60b7-4062-aaff-4585deeb14d4",
   "metadata": {},
   "source": [
    "Anhand der **PdfReader** Bibliothek extrahieren wir den Text aus dem Dokument. Jede Seite wird einzeln gelesen und als Textblock gespeichert. Dies ermöglicht es, das Dokument seitenweise zu analysieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea8224c-e7cb-43e5-9c17-be3116911cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the pdf reader \n",
    "reader = PdfReader(pdf_bytes)\n",
    "\n",
    "# extract pages from pdf document\n",
    "pages = [page.extract_text() for page in reader.pages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98e1344-c22c-420a-aff5-ef049df9b8ce",
   "metadata": {},
   "source": [
    "Wir geben die Gesamtanzahl der Seiten im Dokument aus, um sicherzustellen, dass der Text vollständig eingelesen wurde. Dies dient auch als grundlegende Validierung des Eingabeformats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a892a1-c38b-47ca-b870-187d4539f003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of pages\n",
    "print(f\"Das Dokument umfasst insgesamt {len(pages)} Seiten.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10494ae9-731a-4fd6-8f45-5f8072a6b5cd",
   "metadata": {},
   "source": [
    "Zusätzlich werfen wir einen Blick auf eine spezifische Seite des ursprünglichen Dokuments, um den Text vor der Verarbeitung zu überprüfen. Dies ermöglicht es uns, den extrahierten Text mit der Originalquelle zu vergleichen und sicherzustellen, dass keine Inhalte verloren gegangen oder falsch formatiert wurden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa152c1-346d-4fdf-bf9d-26246f6721e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pages[2][0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b34913-ff17-401c-b0c6-6bb29edf743a",
   "metadata": {},
   "source": [
    "Um die Daten für die spätere Analyse vorzubereiten, wird der Text in kleinere Abschnitte (Chunks) aufgeteilt. Jeder Abschnitt hat eine maximale Länge von 500 Zeichen, wobei eine Überlappung von 50 Zeichen sicherstellt, dass der Kontext zwischen den Abschnitten erhalten bleibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e84d39-1bde-4408-8681-600a257fa8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the character text splitter\n",
    "text_splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "# split text for better indexing\n",
    "documents = [chunk for page in pages for chunk in text_splitter.split_text(page)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c218503c-db3c-4f55-85bf-c01d936603f9",
   "metadata": {},
   "source": [
    "Um sicherzustellen, dass der Text korrekt extrahiert und in kleinere Abschnitte aufgeteilt wurde, werfen wir einen Blick auf den ersten Abschnitt des aufbereiteten Dokuments. Dies hilft, die Struktur und den Inhalt der vorbereiteten Daten zu überprüfen, bevor wir mit der weiteren Analyse fortfahren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083082a1-c53d-4ad7-8b34-3b324ee7a936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print initial document\n",
    "print(documents[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b365d25d-4f23-48b0-9a7d-c449a9e04af7",
   "metadata": {},
   "source": [
    "Diese Strukturierung verbessert die Effizienz und Genauigkeit der Modellanalyse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ea4961-12e0-4201-8c33-8930b988cf72",
   "metadata": {},
   "source": [
    "### 4.2 Embedding der Berichtsinhalte und Foundation Modell Initialisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72181dd-3267-4638-9765-4a14516275b3",
   "metadata": {},
   "source": [
    "In diesem Abschnitt lernen wir, wie Text-Embeddings erstellt und ein Foundation-Modell initialisiert werden. Embeddings sind numerische Repräsentationen von Texten, die es dem Modell ermöglichen, semantische Zusammenhänge zu erkennen. Diese Embeddings werden anschliessend in einem Vektor-Store gespeichert, um relevante Informationen effizient abrufen zu können. Abschliessend richten wir ein leistungsfähiges Foundation-Modell ein, um den Text weiter zu analysieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9452af47-c24d-4443-a1b1-9297ffe95f5e",
   "metadata": {},
   "source": [
    "Wir nutzen das Modell `sentence-transformers/all-MiniLM-L6-v2`, um Text-Embeddings zu erstellen. Dieses Modell ist effizient und kann lokal verwendet werden, ohne zusätzliche Kosten zu verursachen. Die Embeddings bilden die Grundlage für die spätere Analyse des Dokuments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0be809-0bee-43d8-9b1d-8b102eb522c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings and index\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e12a6a7-3e33-410c-b289-3789d49b2a8a",
   "metadata": {},
   "source": [
    "Nachdem die Embeddings erstellt wurden, speichern wir diese in einer Vektor-Datenbank. Diese Datenbank ermöglicht es uns, schnell und präzise auf relevante Abschnitte des Dokuments zuzugreifen. Hier verwenden wir die **Facebook AI Similarity Search (FAISS)** Bibliothek, die speziell für die effiziente Verarbeitung grosser Mengen von Vektordaten optimiert ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a054970-62a3-42ff-b0e1-6799cee6e518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the vector store of document embeddings\n",
    "vectorstore = FAISS.from_texts(documents, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeb64a8-09ac-4d34-8d5f-fd4a659ca709",
   "metadata": {},
   "source": [
    "Anschliessend können wir die Vector-Datenbank dazu verwenden um beliebige Anfragen zu testen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df43b03-649e-4bfe-ad1c-8159d8088902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init an arbitrary query\n",
    "query = 'Bereits in der ersten Fassung des Finanzkontrollgesetzes (FKG) von 1967 war diese Aufgabe enthalten'\n",
    "\n",
    "# retrieve similiar documents\n",
    "similar_docs = vectorstore.similarity_search(query, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7384f57-1653-4a51-9d56-4cde955b3b6f",
   "metadata": {},
   "source": [
    "Im letzten Schritt initialisieren wir das benötigte Foundation-Modell. Der Parameter temperature steuert die Kreativität der generierten Antworten: Ein niedriger Wert sorgt für präzisere und konsistentere Ergebnisse. Die Integration eines Callback-Handlers erlaubt es, den Fortschritt der Modelloperationen in Echtzeit nachzuvollziehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7991d7-da24-4e83-b402-0ec903f71696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the local llama model\n",
    "llm = OllamaLLM(model='llama3:latest', temperature=1.0, callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506ea3e8-c8fd-4627-84d5-cac838124c67",
   "metadata": {},
   "source": [
    "### 4.3 Ausführen der Berichtsanalyse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90a069f-c29b-40d6-85d0-bf42db85949d",
   "metadata": {},
   "source": [
    "Die Berichtsanalyse beginnt mit der Eingabe einer Frage (Query) durch den Benutzer, welche mithilfe eines Embedding-Modells in eine numerische Repräsentation (Query Representation) umgewandelt wird. Diese Vektorrepräsentation wird anschließend mit den gespeicherten Embeddings in der Vektordatenbank abgeglichen, wobei eine Ähnlichkeitssuche (Similarity Search) die relevantesten Informationen aus der Datenbank zurückgibt. \n",
    "\n",
    "Der gefundene Kontext (Query Context) wird daraufhin zusammen mit der ursprünglichen Frage an ein Foundation-Modell weitergeleitet, welches auf Basis der bereitgestellten Informationen eine präzise und kontextbezogene Antwort generiert. Das Schaubild verdeutlicht diesen Ablauf, indem es die Transformation der Benutzeranfrage, die Nutzung der Vektordatenbank und die Integration des Foundation-Modells zur finalen Beantwortung zeigt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8879b647-44ce-43f2-b976-7dd49bb609cb",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 1000px\" src=\"./retrieval_process.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7a3501-11ad-48ef-9b30-21b4b076a7cc",
   "metadata": {},
   "source": [
    "In diesem Abschnitt lernen wir, wie ein Foundation-Modell verwendet wird, um gezielt Fragen an einen Bericht zu stellen und präzise Antworten zu erhalten. Dazu definieren wir eine Frage-Antwort-Kette (QA-Chain), die relevante Textabschnitte kombiniert und analysiert. Mit Hilfe eines Prompts wird das Modell so gesteuert, dass es verständliche und strukturierte Antworten liefert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efed9a9-9e64-42f5-a347-2a6963e64abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a simple prompt for answering questions\n",
    "qa_prompt = PromptTemplate(\n",
    "    input_variables=['context', 'question'],\n",
    "    template='Use the following context to answer the question:\\n\\n{context}\\n\\nquestion: {question}\\nanswer:'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5088124-63fc-48b6-9fe9-8b947a7390db",
   "metadata": {},
   "source": [
    "Die QA-Chain wird mit dem LLaMA-Modell geladen. Diese Kette ermöglicht es, Informationen aus mehreren Textabschnitten zu kombinieren und dadurch umfassendere Antworten zu generieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbec9c9-0f0c-4181-b0c4-7edcbb21b8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the qa chain with the llama model\n",
    "combine_documents_chain = load_qa_chain(llm, chain_type='stuff', prompt=qa_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aa3283-c368-452c-b1ed-69f4909a5fc6",
   "metadata": {},
   "source": [
    "Nun wird die Retrieval-QA-Chain erstellt. Diese kombiniert die Fähigkeit, relevante Textabschnitte mithilfe des Vektor-Stores abzurufen, mit der QA-Chain, um die besten Antworten zu generieren. Dies ist besonders nützlich bei der Analyse grosser Dokumente, da so nur die relevanten Informationen berücksichtigt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c099b83-bd72-4bdd-99ba-39cfc226bbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the retrieval qa chain\n",
    "retrieval_chain = RetrievalQA(\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    combine_documents_chain=combine_documents_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4bf833-e7b4-4f96-9d23-beecf44bfb63",
   "metadata": {},
   "source": [
    "Wir definieren beispielhafte Fragen, um die Funktionalität der QA-Chain zu testen. Diese Fragen decken wichtige Aspekte des Berichts ab, wie Hauptpunkte, Risiken und Empfehlungen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7580f7-8798-4831-9555-d9310d43eba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example questions\n",
    "questions = [\n",
    "    'Was sind die wichtigsten Ergebnisse des Berichts? (bitte sehr kurz und auf deutsch)',\n",
    "    'Was sind die drei wesentlichen Punkte im Bereich Öffentliche Finanzen, Steuern, Credit Suisse und COVID-19? (bitte sehr kurz und auf deutsch)',\n",
    "    'Was sind die drei wesentlichen Punkte im Bereich Wirtschaft und Arbeitsmarkt? (bitte sehr  kurz und auf deutsch)',\n",
    "    'Was sind die drei wesentlichen Punkte im Bereich Gesundheit, soziale Vorsorge und Sport? (bitte sehr kurz und auf deutsch))'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fad166-84b3-466b-8c77-c951a208f475",
   "metadata": {},
   "source": [
    "Zum Schluss stellen wir die definierten Fragen an die Retrieval-QA-Chain. Das Modell liefert präzise Antworten, die auf den relevanten Abschnitten des Berichts basieren. So können zentrale Informationen aus dem Dokument extrahiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2f21c6-58ca-4f25-8176-a07a5786cda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask questions\n",
    "for question in questions:\n",
    "    print(f\"Frage: {question}\\n\\nAntwort:\\n\")\n",
    "    answer = retrieval_chain.invoke(question)\n",
    "    print(f\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1526fd0-41d9-4f88-a698-76be99bddcb1",
   "metadata": {},
   "source": [
    "## Lab Zusammenfassung:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9a741b-2c10-45bd-af5e-cd5373b7b10a",
   "metadata": {},
   "source": [
    "Im Mittelpunkt des Labs standen die folgenden Erkenntnisse:\n",
    "\n",
    "> 1. **Verständnis von Chain-of-Thought Prompting:** Die grundlegenden Konzepte des Chain-of-Thought Prompting wurden erarbeitet, um die Nachvollziehbarkeit und Effizienz von Prüfungsaufgaben zu verbessern.  \n",
    "> 2. **Erstellung von Prompts mit LangChain:** Es wurde gezeigt, wie spezifische Prompts definiert und organisiert werden, darunter Task-, Action-, Input- und Output-Explanation-Prompts sowie globale und lokale Anomaliebeispiele.  \n",
    "> 3. **Implementierung eines Prüfungsworkflows mit LangChain:** Praktische Erfahrungen mit dem Einsatz eines Foundation Modells wie LLaMA und LangChain für Prüfungsaufgaben wurden gesammelt.  \n",
    "> 4. **Analyse und Automatisierung von Prüfungsaufgaben:** Prüfungsaufgaben wie die Anomalieerkennung oder die Berichterstellung wurden mithilfe von KI-gestützten Workflows automatisiert.  \n",
    "\n",
    "Das Lab bot Einblicke in die Anwendung von Foundation Modellen und Chain-of-Thought Prompting für die Finanzprüfung. Durch die schrittweise Definition und Ausführung der Prompts wurden Fähigkeiten zur systematischen und transparenten Gestaltung komplexer Auditprozesse vermittelt."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
